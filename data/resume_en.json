{
  "name": "Pablo Ratache Rojas",
  "title": "Data Engineer",
  "portfolioUrl": "linkedin.com/in/pablo-ratache-rojas-9a9602140/ \u2022 pr0nel.github.io/cv_pablo_ratache/",
  "githubProfileUrl": "https://github.com/Pr0nel",
  "about": {
    "summary": "with a background in Physical Engineering, I specialize in building scalable and efficient data solutions. With solid experience in data warehousing, ETL/ELT processes, and cloud development, I seek to contribute to innovative projects leveraging Python, SQL, PySpark, dbt and platforms like GCP, AWS, Airflow, Databricks, and Snowflake.",
    "professionalProfile": "I designed ETL/ELT pipelines in cloud environments (GCP/AWS) with Apache Airflow, with a focus on Data Governance and cost reduction. I implemented ML-based OCR solutions for voucher processing with 90% accuracy. My background in Physical Engineering allows me to tackle complex multidisciplinary problems and design scalable, efficient data architectures.",
    "image": "./assets/profile.webp"
  },
  "contact": {
    "email": "pablo.ratache@uni.pe",
    "phone": "+51 966979127"
  },
  "education": [
    {
      "degree": "B.Sc. with a major in Physical Engineering",
      "institution": "Universidad Nacional de Ingenieria \u2022 Rimac - Lima - Peru",
      "dates": "2019",
      "details": "Final project involved simulating energy consumption for heating an Experimental Housing Module located at 4500 masl. The temperature predictions are compared with recorded indoor temperatures to validate the model. The target indoor temperature is then calculated. Finally, the energy required to maintain a neutral indoor temperature is calculated."
    }
  ],
  "publications": [
    {
      "title": "Simulation of Energy Consumption for the Heating of an Experimental Housing Module Located at 4500 masl",
      "journal": "TECNIA",
      "date": "Jun 2021",
      "doiUrl": "https://doi.org/10.21754/tecnia.v21i1.1083"
    }
  ],
  "certifications": [],
  "languages": [
    { "language": "Spanish", "level": "Native", "progress": 100 },
    { "language": "English", "level": "Intermediate", "progress": 50 }
  ],
  "experience": [
    {
      "role": "Data Engineer",
      "company": "Konexa - WOM \u2022 Chile (Remote) \u2022 Telecom, B2B/B2C",
      "dates": "February 2025 - Present",
      "description": [
        "KEY ACCOMPLISHMENTS:",
        "- Reduced historical data loading times by 40% in Oracle to BigQuery migration.",
        "- Migrated historical data from Oracle to BigQuery maintaining 100% data integrity.",
        "- Developed modular Machine Learning solution for voucher processing with 90% accuracy.",
        "",
        "RESPONSIBILITIES:",
        "- Documented data lineage mapping from Oracle/SandBox/BigQuery sources to final reports, ensuring traceability for compliance audits.",
        "- Designed and developed ETL/ELT pipelines to migrate historical data from Oracle to BigQuery on GCP, with automated pipeline orchestration and health monitoring using Cloud Composer.",
        "- Implemented a scalable GCP architecture using a layered model to ensure governance and scalability; established a Data Lake in Cloud Storage for raw data and a Data Warehouse in BigQuery for transformed data storage, with Stored Procedures for optimized querying.",
        "- Rewrote and optimized PL/SQL transformation logic (migrated from Oracle) in BigQuery, implementing robust validations to ensure Data Completeness.",
        "- Collaborated with data analytics teams to develop dashboards using Power BI and Looker Studio, delivering SQL views, compliance reports, and technical documentation.",
        "- Applied data governance and security best practices, ensuring all data access complied with information protection policies and access controls.",
        "- Applied partitioning and clustering across the Data Warehouse to optimize query performance, resulting in reduced compute slot costs and increased operational efficiency.",
        "- Configured data infrastructure in GCP by defining BigQuery schemas in JSON and Terraform configurations (tfvars), automating deployment through CI/CD pipelines in GitLab.",
        "- Delivered final data products, including SQL views, compliance reports, and technical documentation using Confluence, with task tracking in Jira.",
        "- Developed and tested ETL/ELT pipelines in AWS (S3, EC2, PySpark) using Jupyter Notebooks for interactive development, translating complex SQL queries into scalable PySpark code.",
        "- Developed and orchestrated distributed PySpark jobs on AWS EMR, using Apache Airflow DAGs."
      ]
    },
    {
      "role": "Production Researcher | Process Engineer",
      "company": "Productora De Alimentos Uno S.A.C. \u2022 Ate-Lima-Peru \u2022 Food Processing, B2B",
      "dates": "May 2024 - January 2025",
      "description": [
        "- Implemented OEE KPI system alongside Production and Operational Excellence teams, establishing the first operational efficiency measurement.",
        "- Established statistical control methodology with Cp and Cpk indices in coordination with Quality team, creating the first process capability monitoring system.",
        "- Supported operational data loading and structuring in SQL Server for KPI reporting.",
        "- Defined reprocessing limits to optimize fermentation times and reduce waste."
      ]
    },
    {
      "role": "Data Engineer",
      "company": "BCP - Yape \u2022 Peru (Remote) \u2022 Fintech, B2B/B2C",
      "dates": "February 2024 - April 2024",
      "description": [
        "- Implemented serverless functions with Cloud Functions for real-time data processing.",
        "- Developed custom Python scripts for data validation and bulk cleansing.",
        "- Automated repetitive tasks in Excel with advanced formulas and macros, increasing operational efficiency in quality control processes."
      ]
    }
  ],
  "projects": [
    {
      "title": "Voucher Processing with OCR and Machine Learning",
      "description": "I designed a Python application that processes voucher images (or receipts) through a pipeline that includes segmenting the vouchers within a larger image, validating whether the segments are actually vouchers and extracting text using OCR.",
      "image": "assets/project1.webp",
      "repositoryUrl": "https://github.com/Pr0nel/proyecto_ocr_vouchers"
    },
    {
      "title": "Data Analysis and Modeling for Solar Radiation Prediction",
      "description": "Data analysis and modeling project to predict hourly solar radiation over a given time horizon. This project involves data exploration, data preprocessing and cleaning, and training a machine learning model to perform the prediction.",
      "image": "assets/project2.webp",
      "repositoryUrl": "https://github.com/Pr0nel/weather_data"
    },
    {
      "title": "ETL/ELT Pipeline: Medallion Architecture in Snowflake",
      "description": "End-to-end data pipeline implementing Medallion Architecture in Snowflake that transforms retail orders from unstandardized Excel files into business-ready analytical views. This includes automated ingestion, quality validation, key metrics and modular configuration via YAML for scalability.",
      "image": "assets/project3.webp",
      "repositoryUrl": "https://github.com/Pr0nel/snippf-poc"
    }
  ],
  "skills": {
    "languages": [
      { "name": "C/C++", "level": 100 },
      { "name": "Python", "level": 100 },
      { "name": "SQL", "level": 100 },
      { "name": "PySpark", "level": 100 },
      { "name": "Bash", "level": 80 },
      { "name": "PowerShell", "level": 80 },
      { "name": "Java", "level": 70 },
      { "name": "MATLAB", "level": 70 },
      { "name": "HTML", "level": 70 },
      { "name": "CSS", "level": 70 },
      { "name": "JavaScript", "level": 70 }
    ],
    "tools": [
      {
        "name": "Project management tools: Slack, Microsoft Teams, Trello, Jira, Confluence",
        "level": 100
      },
      {
        "name": "Data analysis & visualization tools: Microsoft Excel, Power BI, Looker Studio",
        "level": 80
      },
      {
        "name": "Version control: Git, GitHub, GitLab, Bitbucket",
        "level": 100
      },
      {
        "name": "Database: OLTP (PostgreSQL), OLAP (BigQuery, Snowflake, Amazon Redshift)",
        "level": 100
      },
      {
        "name": "CI/CD & Infrastructure as Code: GitHub Actions, GitLab, Terraform (IaC)",
        "level": 90
      },
      {
        "name": "Containers, virtualization & operating systems: Docker, Docker Compose, VirtualBox, VMware, Linux, Windows",
        "level": 80
      }
    ],
    "concepts": [
      { "name": "Fast learner", "level": 100 },
      { "name": "Strategic planning", "level": 100 },
      { "name": "Team player", "level": 100 },
      { "name": "Adaptability", "level": 100 },
      { "name": "Pragmatic thinking", "level": 100 },
      { "name": "Abstract thinking", "level": 100 },
      { "name": "Active listening", "level": 100 },
      { "name": "Problem solving", "level": 100 },
      { "name": "Time management", "level": 100 },
      { "name": "Change management", "level": 100 },
      { "name": "Project management and agile methodologies: PMBOK, 5S, Scrum, Kanban, Design Thinking, Lean mindset", "level": 100 },
      { "name": "Quality & efficiency metrics: Data Completeness, Data Consistency, OEE, Cp, Cpk", "level": 100 },
      { "name": "Data Warehousing: BigQuery, Snowflake, Amazon Redshift", "level": 100 },
      { "name": "ETL & Orchestration: Databricks, Airflow", "level": 100 },
      { "name": "Data Modeling: dbt", "level": 100 },
      { "name": "Big Data & Analytics: Apache Spark", "level": 80 },
      { "name": "Cloud Platforms: Amazon Web Services, Google Cloud Platform", "level": 80 },
      { "name": "Machine Learning & Computer Vision: CLIP, SAM, AWS Textract, OCR", "level": 80 }
    ]
  }
}
