{
  "name": "Pablo Ratache Rojas",
  "title": "Data Engineer",
  "portfolioUrl": "linkedin.com/in/pablo-ratache-rojas-9a9602140/ \u2022 pr0nel.github.io/cv_pablo_ratache/",
  "githubProfileUrl": "https://github.com/Pr0nel",
  "about": {
    "summary": "with a background in Physical Engineering, I specialize in building scalable and efficient data solutions. With solid experience in data warehousing, ETL/ELT processes, and cloud development, I seek to contribute to innovative projects leveraging Python, SQL, PySpark, dbt and platforms like GCP, AWS, Airflow, Databricks, and Snowflake.",
    "professionalProfile": "I designed ETL/ELT pipelines in cloud environments (GCP/AWS) with Apache Airflow, reducing data load times by 40%. I implemented ML-based OCR solutions for voucher processing with 90% accuracy. My background in Physical Engineering allows me to tackle complex multidisciplinary problems and design scalable, efficient data architectures.",
    "image": "./assets/profile.webp"
  },
  "contact": {
    "email": "pablo.ratache@uni.pe",
    "phone": "+51 966979127"
  },
  "education": [
    {
      "degree": "B.Sc. with a major in Physical Engineering",
      "institution": "Universidad Nacional de Ingenieria \u2022 Rimac - Lima - Peru",
      "dates": "2019",
      "details": "Final project involved simulating energy consumption for heating an Experimental Housing Module located at 4500 masl. The temperature predictions are compared with recorded indoor temperatures to validate the model. The target indoor temperature is then calculated. Finally, the energy required to maintain a neutral indoor temperature is calculated."
    }
  ],
  "publications": [
    {
      "title": "Simulation of Energy Consumption for the Heating of an Experimental Housing Module Located at 4500 masl",
      "journal": "TECNIA",
      "date": "Jun 2021",
      "doiUrl": "https://doi.org/10.21754/tecnia.v21i1.1083"
    }
  ],
  "certifications": [],
  "languages": [
    { "language": "Spanish", "level": "Native", "progress": 100 },
    { "language": "English", "level": "Intermediate", "progress": 50 }
  ],
  "experience": [
    {
      "role": "Data Engineer",
      "company": "Konexa - WOM \u2022 Chile (Remote) \u2022 Telecom, B2B/B2C",
      "dates": "February 2025 - Present",
      "description": [
        "KEY ACCOMPLISHMENTS:",
        "- Reduced historical data loading times by 40% in Oracle to BigQuery migration through ETL/ELT pipeline automation using Apache Airflow DAG orchestration.",
        "- Migrated historical data from Oracle to BigQuery maintaining 100% data integrity.",
        "- Deployed a Docker container (Linux + PostgreSQL) for web-scraping ETL, normalizing records and reducing latency by 60%.",
        "- Developed modular Machine Learning solution (SAM, CLIP, OCR, AWS Textract) for voucher processing with 90% accuracy.",
        "",
        "RESPONSIBILITIES:",
        "- Documented complete data lineage mapping from Oracle/SandBox/BigQuery sources to final reports, documenting traceability for compliance audits.",
        "- Configured data infrastructure in GCP by defining BigQuery schemas in JSON and Terraform configurations (tfvars), automating deployment through CI/CD in GitLab.",
        "- Delivered final data products, including SQL views, compliance reports, technical documentation, and RFCs using Confluence, with CI/CD in GitLab and tracking in Jira.",
        "- Developed and tested ETL/ELT pipelines on AWS (S3, EC2, PySpark) with Jupyter Notebooks for interactive development and complex SQL to scalable PySpark code translation.",
        "- Orchestrated distributed PySpark jobs on AWS EMR through Apache Airflow DAGs, ensuring scheduled executions, retries, and status monitoring."
      ]
    },
    {
      "role": "Production Researcher | Process Engineer",
      "company": "Productora De Alimentos Uno S.A.C. \u2022 Ate-Lima-Peru \u2022 Food Processing, B2B",
      "dates": "May 2024 - January 2025",
      "description": [
        "- Implemented OEE KPI system alongside Production and Operational Excellence teams, establishing the first operational efficiency measurement.",
        "- Established statistical control methodology with Cp and Cpk indices in coordination with Quality team, creating the first process capability monitoring system.",
        "- Supported operational data loading and structuring in SQL Server for production KPI reporting.",
        "- Performed tank and equipment calibration, ensuring metrological traceability.",
        "- Defined reprocessing limits to optimize fermentation times and reduce waste."
      ]
    },
    {
      "role": "Data Engineer",
      "company": "BCP - Yape \u2022 Peru (Remote) \u2022 Fintech, B2B/B2C",
      "dates": "February 2024 - April 2024",
      "description": [
        "- Created Cloud Functions to execute serverless functions that respond to real-time data events.",
        "- Developed custom Python scripts for data validation and bulk cleansing.",
        "- Automated repetitive tasks in Excel with advanced formulas and macros, increasing operational efficiency in quality control processes."
      ]
    }
  ],
  "projects": [
    {
      "title": "Voucher Processing with OCR and Machine Learning",
      "description": "I designed a Python application that processes voucher images (or receipts) through a pipeline that includes segmenting the vouchers within a larger image, validating whether the segments are actually vouchers and extracting text using OCR.",
      "image": "assets/project1.webp",
      "repositoryUrl": "https://github.com/Pr0nel/proyecto_ocr_vouchers"
    },
    {
      "title": "Data Analysis and Modeling for Solar Radiation Prediction",
      "description": "Data analysis and modeling project to predict hourly solar radiation over a given time horizon. This project involves data exploration, data preprocessing and cleaning, and training a machine learning model to perform the prediction.",
      "image": "assets/project2.webp",
      "repositoryUrl": "https://github.com/Pr0nel/weather_data"
    },
    {
      "title": "ETL/ELT Pipeline: Medallion Architecture in Snowflake",
      "description": "End-to-end data pipeline implementing Medallion Architecture in Snowflake that transforms retail orders from unstandardized Excel files into business-ready analytical views. This includes automated ingestion, quality validation, key metrics and modular configuration via YAML for scalability.",
      "image": "assets/project3.webp",
      "repositoryUrl": "https://github.com/Pr0nel/snippf-poc"
    }
  ],
  "skills": {
    "languages": [
      { "name": "C/C++", "level": 100 },
      { "name": "Python", "level": 100 },
      { "name": "SQL", "level": 100 },
      { "name": "PySpark", "level": 100 },
      { "name": "Bash", "level": 80 },
      { "name": "PowerShell", "level": 80 },
      { "name": "Java", "level": 70 },
      { "name": "MATLAB", "level": 70 },
      { "name": "HTML", "level": 70 },
      { "name": "CSS", "level": 70 },
      { "name": "JavaScript", "level": 70 }
    ],
    "tools": [
      {
        "name": "Project management tools: Slack, Microsoft Teams, Trello, Jira, Confluence",
        "level": 100
      },
      {
        "name": "Database: OLTP (PostgreSQL), Analytical / OLAP workloads (BigQuery, Snowflake)",
        "level": 100
      },
      {
        "name": "Cloud Platforms, CI/CD & Infrastructure as Code: GCP, AWS, GitHub Actions, GitLab, Terraform (IaC)",
        "level": 90
      },
      {
        "name": "Version control: Git, GitHub, GitLab, Bitbucket",
        "level": 100
      },
      {
        "name": "Data analysis & visualization tools: Microsoft Excel, Power BI, Looker Studio",
        "level": 80
      },
      {
        "name": "Containers, virtualization & operating systems: Docker, Docker Compose, VirtualBox, VMware, Linux, Windows",
        "level": 80
      }
    ],
    "concepts": [
      { "name": "Fast learner", "level": 100 },
      { "name": "Strategic planning", "level": 100 },
      { "name": "Team player", "level": 100 },
      { "name": "Adaptability", "level": 100 },
      { "name": "Pragmatic thinking", "level": 100 },
      { "name": "Abstract thinking", "level": 100 },
      { "name": "Active listening", "level": 100 },
      { "name": "Problem solving", "level": 100 },
      { "name": "Time management", "level": 100 },
      { "name": "Change management", "level": 100 },
      { "name": "Project management and agile methodologies: PMBOK, 5S, Scrum, Kanban, Design Thinking, Lean mindset", "level": 100 },
      { "name": "Quality & efficiency metrics: Data Completeness, Data Consistency, OEE, Cp, Cpk", "level": 100 },
      { "name": "Data Warehousing: BigQuery, Snowflake, Amazon Redshift", "level": 100 },
      { "name": "ETL & Orchestration: Databricks, Airflow", "level": 100 },
      { "name": "Data Modeling: dbt", "level": 100 },
      { "name": "Big Data & Analytics: Apache Spark", "level": 80 },
      { "name": "Cloud Computing: AWS, GCP", "level": 80 },
      { "name": "Machine Learning & Computer Vision: CLIP, SAM, AWS Textract, OCR", "level": 80 }
    ]
  }
}
