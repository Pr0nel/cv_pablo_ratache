{
  "name": "Pablo Ratache Rojas",
  "title": "Data Engineer & Software Developer",
  "portfolioUrl": "pr0nel.github.io/cv_pablo_ratache/",
  "githubProfileUrl": "https://github.com/Pr0nel",
  "about": {
    "summary": "con formación en Ingeniería Física especializado en construir soluciones escalables y eficientes. Con una base sólida en almacenamiento de datos, procesos ETL y conceptos de programación, estoy deseando contribuir a proyectos innovadores y seguir creciendo profesionalmente. Mis habilidades clave incluyen Python, SQL, Pyspark, y plataformas en la nube como GCP y AWS.",
    "professionalProfile": "+1 año de experiencia en diseño de pipelines ETL/ELT en entornos cloud (GCP / AWS), usando Apache Airflow, reduciendo tiempos de carga en 40 %. Tengo experiencia en desarrollo de soluciones ML/OCR con 90 % de precisión, metodologías ágiles y cumplimiento de estándares de calidad, desarrollo web y proyectos independientes. Background en Ingeniería Física me aporta habilidades como la descomposición sistemática de problemas y la modelización de sistemas complejos, para el diseño de arquitecturas de datos escalables y la resolución eficiente de desafíos técnicos en entornos productivos.",
    "image": "assets/profile.webp"
  },
  "contact": {
    "email": "pablo.ratache@uni.pe",
    "phone": "+51 966979127"
  },
  "education": [
    {
      "degree": "B.Sc. con mención en Ingeniería Física",
      "institution": "Universidad Nacional de Ingeniería \u2022 Rímac - Lima - Perú",
      "dates": "2010 - 2019",
      "details": "El proyecto final consistió en simular el consumo de energía para la calefacción de un Módulo de Vivienda Experimental ubicado a 4500 msnm. Las predicciones de temperatura se comparan con las temperaturas interiores registradas, para validar el modelo. A continuación, se calcula la temperatura interior objetivo. Finalmente, se calcula la energía necesaria para mantener una temperatura interior neutra."
    }
  ],
  "publications": [
    {
      "title": "Simulación del consumo energético para la calefacción de un módulo de vivienda experimental ubicado a 4500 msnm",
      "journal": "TECNIA",
      "date": "Jun 2021",
      "doiUrl": "https://doi.org/10.21754/tecnia.v21i1.1083"
    }
  ],
  "certifications": [],
  "languages": [
    { "language": "Español", "level": "Nativo", "progress": 100 },
    { "language": "Inglés", "level": "Intermedio", "progress": 50 }
  ],
  "experience": [
    {
      "role": "Jr. Data Engineer",
      "company": "Konexa - WOM \u2022 Chile (Remoto) \u2022 Telecomunicaciones, B2C/B2B",
      "dates": "Febrero 2025 - Actualidad",
      "description": [
        "LOGROS CLAVE:",
        "- Reduje tiempos de carga de datos históricos en 40 % en la migración de Oracle a BigQuery, con la automatización de pipelines ETL/ELT mediante la orquestación de DAGs en Apache Airflow.",
        "- Migré datos históricos de Oracle a BigQuery manteniendo 100% de integridad.",
        "- Desplegué contenedor Docker (Linux + PostgreSQL) para ETL web-scraping, normalizando registros y reduciendo latencia en 60 %.",
        "- Desarrollé solución modular de Machine Learning (SAM, CLIP, OCR, AWS Textract) para procesamiento de vouchers con 90 % de precisión.",
        "",
        "RESPONSABILIDADES:",
        "- Documenté mapeo completo del linaje de datos desde fuentes en Oracle/SandBox/BigQuery hasta reportes finales, documentando trazabilidad para auditorías de compliance.",
        "- Configuré infraestructura de datos en GCP definiendo esquemas BigQuery en JSON y configuraciones Terraform (tfvars), automatizando despliegue mediante CI/CD en GitLab.",
        "- Entregué productos de datos finales: vistas SQL, reportes de compliance, documentación técnica y RFCs usando Confluence, con CI/CD en GitLab y seguimiento en Jira.",
        "- Desarrollé y probé pipelines ETL/ELT en AWS (S3, EC2, PySpark) con Jupyter Notebooks para desarrollo interactivo y traducción de SQL complejas a código PySpark escalable.",
        "- Orquesté jobs distribuidos de PySpark en AWS EMR mediante DAGs de Apache Airflow, asegurando ejecuciones programadas, reintentos y monitoreo de estado."
      ]
    },
    {
      "role": "Investigador de Producción",
      "company": "Productora De Alimentos Uno S.A.C. \u2022 Ate-Lima-Perú \u2022 Alimentos Procesados, B2B",
      "dates": "Mayo 2024 - Enero 2025",
      "description": [
        "- Implementé sistema de KPIs de OEE junto al equipo de Producción y Excelencia Operacional, estableciendo la primera medición de eficiencia operacional.",
        "- Establecí metodología de control estadístico con índices Cp y Cpk en coordinación con el equipo de Calidad, creando el primer sistema de monitoreo de capacidad de procesos.",
        "- Apoyé en la carga y estructuración de datos operativos en SQL Server, para reportes de KPIs de producción.",
        "- Realicé la calibración de tanques y equipos, asegurando trazabilidad metrológica.",
        "- Definí límites de reproceso para optimizar tiempos de fermentación y reducir mermas."
      ]
    },
    {
      "role": "Asistente de Calibraciones",
      "company": "Factio Qualitas S.A.C. \u2022 Surco-Lima-Perú \u2022 Consultora Farmacéutica, B2B/B2C",
      "dates": "Junio 2023 - Enero 2024",
      "description": [
        "- Supervisé y mentoré a un colaborador, reduciendo tiempo de rampa en un 50 % y mejorando la consistencia de sus entregas a 80 %.",
        "- Implementé un sistema centralizado de gestión de clientes, equipos y certificados, con cloud storage y data governance, reduciendo el tiempo del servicio en 60 %.",
        "- Apliqué enfoque data-driven para análisis de resultados y toma de decisiones.",
        "- Coordiné con proveedores externos para la calibración en campo, asegurando el cumplimiento técnico, normativo y de cronogramas.",
        "- Colaboré con QA/QC en la mejora continua del sistema de gestión del laboratorio.",
        "- Apoyé en documentación para acreditación ISO/IEC 17025."
      ]
    }
  ],
  "projects": [
    {
      "title": "Procesamiento de cupones con OCR y aprendizaje automático",
      "description": "Diseñé una aplicación Python que permite el proceso imágenes de vouchers (comprobantes o recibos) a través de un pipeline que incluye segmentación de los vouchers dentro de una imagen más grande, la validación si los segmentos son realmente vouchers y la extracción de texto mediante OCR.",
      "image": "assets/project1.webp",
      "repositoryUrl": "https://github.com/Pr0nel/proyecto_ocr_vouchers"
    },
    {
      "title": "Análisis y Modelado de Datos para Predicción de Radiación Solar",
      "description": "Proyecto de análisis y modelado de datos para predecir la radiación solar horaria en un horizonte de tiempo determinado. Este proyecto involucra la exploración de datos, el preprocesamiento y limpieza de los mismos, y el entrenamiento de un modelo de aprendizaje automático para realizar la predicción.",
      "image": "assets/project2.webp",
      "repositoryUrl": "https://github.com/Pr0nel/weather_data"
    },
    {
      "title": "Pipeline ETL/ELT: Ingesta y Normalización de Datos Web",
      "description": "Desarrollé un pipeline ETL/ELT para la ingesta de datos web (web-scraping), optimizando el proceso de extracción y normalización. Utilicé tecnologías como Python y PostgreSQL para la lógica de datos y Docker para la contenerización y despliegue en un entorno Linux. El proyecto logró normalizar registros diarios y reducir la latencia de procesamiento en un 60%.",
      "image": "assets/project3.webp",
      "repositoryUrl": "https://github.com/Pr0nel/snippf-poc"
    }
  ],
  "skills": {
    "languages": [
      { "name": "C/C++", "level": 100 },
      { "name": "Python", "level": 90 },
      { "name": "SQL", "level": 90 },
      { "name": "PySpark", "level": 80 },
      { "name": "Bash", "level": 80 },
      { "name": "Java", "level": 70 },
      { "name": "HTML", "level": 65 },
      { "name": "CSS", "level": 65 },
      { "name": "JavaScript", "level": 65 }
    ],
    "tools": [
      {
        "name": "Herramientas de gestión de proyectos: Slack, Microsoft Teams, Trello, Jira, Confluence",
        "level": 100
      },
      {
        "name": "Control de versiones: Git, GitHub, GitLab, Bitbucket",
        "level": 100
      },
      {
        "name": "Cloud, CI/CD y Orquestación: GCP, AWS, Apache Airflow, Terraform (IaC)",
        "level": 80
      },
      {
        "name": "Herramientas de análisis y visualización de datos: Microsoft Excel, Power BI, Looker Studio",
        "level": 70
      },
      {
        "name": "Sistemas operativos: Linux, Windows",
        "level": 100
      },
      {
        "name": "Contenedores y Virtualización: Docker, Docker Compose, VirtualBox",
        "level": 80
      },
      {
        "name": "Base de datos: PostgreSQL (OLTP), BigQuery (OLAP)",
        "level": 100
      }
    ],
    "concepts": [
      { "name": "Aprendizaje rápido", "level": 100 },
      { "name": "Planificación estratégica", "level": 100 },
      { "name": "Trabajo en equipo", "level": 100 },
      { "name": "Adaptabilidad", "level": 100 },
      { "name": "Pensamiento pragmático", "level": 100 },
      { "name": "Pensamiento abstracto", "level": 100 },
      { "name": "Escucha activa", "level": 100 },
      { "name": "Resolución de problemas", "level": 100 },
      { "name": "Gestión del tiempo", "level": 100 },
      { "name": "Gestión del cambio", "level": 100 },
      { "name": "Gestión de proyectos y metodologías ágiles: Scrum, Kanban, Design Thinking, enfoque Lean, PMBOK", "level": 100 },
      { "name": "Métricas de calidad y eficiencia: OEE, Cp, Cpk", "level": 100 },
      { "name": "QA/QC y Normativas: ISO/IEC 17025, INACAL", "level": 100 },
      { "name": "Data Warehousing", "level": 100 },
      { "name": "ETL Processes", "level": 100 },
      { "name": "Data Modeling", "level": 100 },
      { "name": "Analytics & Big Data", "level": 100 },
      { "name": "Cloud Computing (AWS/GCP)", "level": 80 },
      { "name": "Machine Learning & Computer Vision: CLIP, SAM, AWS Textract, OCR", "level": 80 }
    ]
  }  
}
