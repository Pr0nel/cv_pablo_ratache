{
  "name": "Pablo Ratache Rojas",
  "title": "Data Engineer",
  "portfolioUrl": "linkedin.com/in/pablo-ratache-rojas-9a9602140/ \u2022 pr0nel.github.io/cv_pablo_ratache/",
  "githubProfileUrl": "https://github.com/Pr0nel",
  "about": {
    "summary": "con formación en Ingeniería Física, me especializo en la creación de soluciones de datos escalables y eficientes. Con una sólida experiencia en almacenamiento de datos, procesos ETL/ELT y desarrollo en la nube, busco contribuir a proyectos innovadores que aprovechen Python, SQL, PySpark, dbt y plataformas como GCP, AWS, Airflow, Databricks y Snowflake.",
    "professionalProfile": "Diseñé pipelines ETL/ELT en entornos de nube (GCP/AWS) con Apache Airflow, con foco en Gobernaza de Datos y reducción de costos. Implementé soluciones de OCR basadas en ML para el procesamiento de cupones con una precisión del 90 %. Mi formación en Ingeniería Física me permite abordar problemas multidisciplinarios complejos y diseñar arquitecturas de datos escalables y eficientes.",
    "image": "./assets/profile.webp"
  },
  "contact": {
    "email": "pablo.ratache@uni.pe",
    "phone": "+51 966979127"
  },
  "education": [
    {
      "degree": "B.Sc. con mención en Ingeniería Física",
      "institution": "Universidad Nacional de Ingeniería \u2022 Rímac - Lima - Perú",
      "dates": "2019",
      "details": "El proyecto final consistió en simular el consumo de energía para la calefacción de un Módulo de Vivienda Experimental ubicado a 4500 msnm. Las predicciones de temperatura se comparan con las temperaturas interiores registradas, para validar el modelo. A continuación, se calcula la temperatura interior objetivo. Finalmente, se calcula la energía necesaria para mantener una temperatura interior neutra."
    }
  ],
  "publications": [
    {
      "title": "Simulación del consumo energético para la calefacción de un módulo de vivienda experimental ubicado a 4500 msnm",
      "journal": "TECNIA",
      "date": "Jun 2021",
      "doiUrl": "https://doi.org/10.21754/tecnia.v21i1.1083"
    }
  ],
  "certifications": [],
  "languages": [
    { "language": "Español", "level": "Nativo", "progress": 100 },
    { "language": "Inglés", "level": "Intermedio", "progress": 50 }
  ],
  "experience": [
    {
      "role": "Data Engineer",
      "company": "Konexa - WOM \u2022 Chile (Remoto) \u2022 Telecomunicaciones, B2C/B2B",
      "dates": "Febrero 2025 - Actualidad",
      "description": [
        "LOGROS CLAVE:",
        "- Reduje tiempos de carga de datos históricos en 40 % en la migración de Oracle a BigQuery.",
        "- 100% de integridad en la migración de datos históricos de Oracle a BigQuery.",
        "- 90 % de precisión en el desarrollo de una solución modular de ML para el procesamiento de vouchers.",
        "",
        "RESPONSABILIDADES:",
        "- Documenté mapeo del linaje de datos desde fuentes en Oracle/SandBox/BigQuery hasta reportes finales, documentando trazabilidad para auditorías de compliance.",
        "- Diseñé y desarrollé pipelines ETL/ELT para migrar datos históricos desde Oracle a BigQuery en GCP, con orquestación automatizada de pipelines y monitoreo de estado con Cloud Composer.",
        "- Implementé una arquitectura en GCP aplicando un modelo de capas para garantizar gobernanza y escalabilidad; con Data Lake en Cloud Storage como fuente de datos crudos, y Data Warehouse en BigQuery como destino para el almacenamiento de datos transformados y Stored Procedures.",
        "- Reescribí y optimicé lógica de transformación PL/SQL (proveniente de Oracle) en BigQuery, implementando validaciones robustas para garantizar la Data Completeness.",
        "- Colaboré con equipos de análisis de datos para desarrollar dashboards con Power BI y Looker Studio, entregando vistas SQL, reportes de compliance y documentación técnica.",
        "- Apliqué prácticas de Data Governance y seguridad, asegurando que todos los accesos a datos cumplan con políticas de acceso y protección de información.",
        "- Apliqué partitioning y clustering en el Data Warehouse para optimizar el rendimiento de las consultas, reduciendo así los costos de slots de cómputo y un aumento de la eficiencia operativa.",
        "- Configuré infraestructura de datos en GCP definiendo esquemas BigQuery en JSON y configuraciones Terraform (tfvars), automatizando despliegue mediante CI/CD en GitLab.",
        "- Entregué vistas SQL y reportes de compliance utilizando Confluence para documentación técnica y seguimiento en Jira.",
        "- Desarrollé y probé pipelines de ETL/ELT en AWS (S3, EC2, PySpark) utilizando Jupyter Notebooks para desarrollo interactivo, traduciendo SQL complejas a código PySpark escalable.",
        "- Desarrollé y orquesté jobs distribuidos de PySpark en AWS EMR, mediante DAGs de Apache Airflow."
      ]
    },
    {
      "role": "Investigador de Producción",
      "company": "Productora De Alimentos Uno S.A.C. \u2022 Ate-Lima-Perú \u2022 Alimentos Procesados, B2B",
      "dates": "Mayo 2024 - Enero 2025",
      "description": [
        "- Implementé sistema de KPIs de OEE junto al equipo de Producción y Excelencia Operacional, estableciendo la primera medición de eficiencia operacional.",
        "- Establecí metodología de control estadístico con índices Cp y Cpk en coordinación con el equipo de Calidad, creando el primer sistema de monitoreo de capacidad de procesos.",
        "- Apoyé en la carga y estructuración de datos operativos en SQL Server, para reportes de KPIs.",
        "- Definí límites de reproceso para optimizar tiempos de fermentación y reducir mermas."
      ]
    },
    {
      "role": "Data Engineer",
      "company": "BCP - Yape \u2022 Perú (Remoto) \u2022 Fintech, B2B/B2C",
      "dates": "Febrero 2024 - Abril 2024",
      "description": [
        "- Implementé funciones serverless con Cloud Functions para el procesamiento de datos en tiempo real.",
        "- Desarrollé scripts personalizados en Python para la validación de datos y limpieza masiva.",
        "- Automatizé tareas repetitivas en Excel con fórmulas avanzadas y macros, aumentando la eficiencia operativa en los procesos de control de calidad."
      ]
    }
  ],
  "projects": [
    {
      "title": "Procesamiento de cupones con OCR y aprendizaje automático",
      "description": "Diseñé una aplicación Python que permite el proceso imágenes de vouchers (comprobantes o recibos) a través de un pipeline que incluye segmentación de los vouchers dentro de una imagen más grande, la validación si los segmentos son realmente vouchers y la extracción de texto mediante OCR.",
      "image": "assets/project1.webp",
      "repositoryUrl": "https://github.com/Pr0nel/proyecto_ocr_vouchers"
    },
    {
      "title": "Análisis y Modelado de Datos para Predicción de Radiación Solar",
      "description": "Proyecto de análisis y modelado de datos para predecir la radiación solar horaria en un horizonte de tiempo determinado. Este proyecto involucra la exploración de datos, el preprocesamiento y limpieza de los mismos, y el entrenamiento de un modelo de aprendizaje automático para realizar la predicción.",
      "image": "assets/project2.webp",
      "repositoryUrl": "https://github.com/Pr0nel/weather_data"
    },
    {
      "title": "Pipeline ETL/ELT: Arquitectura Medallion en Snowflake",
      "description": "Pipeline de datos end-to-end implementando arquitectura Medallion en Snowflake que transforma órdenes de retail desde archivos Excel sin estandarizar hasta vistas analíticas listas para negocio. Mediante ingesta automatizada, validaciones de calidad, métricas clave y configuración modular vía YAML para escalabilidad.",
      "image": "assets/project3.webp",
      "repositoryUrl": "https://github.com/Pr0nel/snowflake-poc"
    }
  ],
  "skills": {
    "languages": [
      { "name": "C/C++", "level": 100 },
      { "name": "Python", "level": 100 },
      { "name": "SQL", "level": 100 },
      { "name": "PySpark", "level": 100 },
      { "name": "Bash", "level": 80 },
      { "name": "PowerShell", "level": 80 },
      { "name": "Java", "level": 70 },
      { "name": "MATLAB", "level": 70 },
      { "name": "HTML", "level": 70 },
      { "name": "CSS", "level": 70 },
      { "name": "JavaScript", "level": 70 }
    ],
    "tools": [
      {
        "name": "Herramientas de gestión de proyectos: Slack, Microsoft Teams, Trello, Jira, Confluence",
        "level": 100
      },
      {
        "name": "Herramientas de análisis y visualización de datos: Microsoft Excel, Power BI, Looker Studio",
        "level": 80
      },
      {
        "name": "Control de versiones: Git, GitHub, GitLab, Bitbucket",
        "level": 100
      },
      {
        "name": "Base de datos: OLTP (PostgreSQL), OLAP (BigQuery, Snowflake, Amazon Redshift)",
        "level": 100
      },
      {
        "name": "CI/CD y Infraestructura como Código: GitHub Actions, GitLab, Terraform (IaC)",
        "level": 90
      },
      {
        "name": "Contenedores, virtualización y sistemas operativos: Docker, Docker Compose, VirtualBox, VMware, Linux, Windows",
        "level": 80
      }
    ],
    "concepts": [
      { "name": "Aprendizaje rápido", "level": 100 },
      { "name": "Planificación estratégica", "level": 100 },
      { "name": "Trabajo en equipo", "level": 100 },
      { "name": "Adaptabilidad", "level": 100 },
      { "name": "Pensamiento pragmático", "level": 100 },
      { "name": "Pensamiento abstracto", "level": 100 },
      { "name": "Escucha activa", "level": 100 },
      { "name": "Resolución de problemas", "level": 100 },
      { "name": "Gestión del tiempo", "level": 100 },
      { "name": "Gestión del cambio", "level": 100 },
      { "name": "Gestión de proyectos y metodologías ágiles: PMBOK, 5S, Scrum, Kanban, Design Thinking, Enfoque Lean", "level": 100 },
      { "name": "Métricas de calidad y eficiencia: Data Completeness, Data Consistency, OEE, Cp, Cpk", "level": 100 },
      { "name": "Almacenamiento de datos: BigQuery, Snowflake, Amazon Redshift", "level": 100 },
      { "name": "ETL y Orquestación: Databricks, Apache Airflow", "level": 100 },
      { "name": "Modelamiento de datos: dbt", "level": 100 },
      { "name": "Big Data & Analytics: Apache Spark", "level": 80 },
      { "name": "Cloud Platforms: Amazon Web Services, Google Cloud Platform", "level": 80 },
      { "name": "Machine Learning & Computer Vision: CLIP, SAM, AWS Textract, OCR", "level": 80 }
    ]
  }  
}